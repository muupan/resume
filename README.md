# Yasuhiro Fujita

- Email: muupan@gmail.com

## Research interests

- Reinforcement learning

## Publications

[Google Scholar](https://scholar.google.com/citations?user=vfOynukAAAAJ)

- T. Xu, **Y. Fujita**, and E. Matsumoto,  “Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis,” CVPR, 2022. [arXiv](https://arxiv.org/abs/2201.01683)
- **Y. Fujita**, P. Nagarajan, T. Kataoka, and T. Ishikawa, “ChainerRL:  ChainerRL: A Deep Reinforcement Learning Library,” Journal of Machine Learning Research, 22(77), 1-14. [arXiv](https://arxiv.org/abs/1912.03905v2) [code](https://github.com/chainer/chainerrl)
- **Y. Fujita**, K. Uenishi, A. Ummadisingu, P. Nagarajan, S. Masuda, and M. Y. Castro, “Distributed Reinforcement Learning of Targeted Grasping with Active Vision for Mobile Manipulators,” IROS, 2020. [arXiv](http://arxiv.org/abs/2007.08082)
- **Y. Fujita**, T. Kataoka, P. Nagarajan, and T. Ishikawa, “ChainerRL: A Deep Reinforcement Learning Library,” in NeurIPS Deep Reinforcement Learning Workshop, 2019. [arXiv](https://arxiv.org/abs/1912.03905v1) [code](https://github.com/chainer/chainerrl)
- A. Havens, Y. Ouyang, P. Nagarajan, and **Y. Fujita**, “Learning Latent State Spaces for Planning through Reward Prediction,” in NeurIPS Deep Reinforcement Learning Workshop, 2019. [arXiv](https://arxiv.org/abs/1912.04201)
- Y. Nagano, S. Yamaguchi, **Y. Fujita**, and M. Koyama, “A Wrapped Normal Distribution on Hyperbolic Space for Gradient-Based Learning,” in ICML, 2019. [arXiv](https://arxiv.org/abs/1902.02992)
- M. Miyashita, S. Maruyama, **Y. Fujita**, M. Kusumoto, T. Pfeiffer, E. Matsumoto, R. Okuta, and D. Okanohara, “Toward Onboard Control System for Mobile Robots via Deep Reinforcement Learning,” in NeurIPS Deep RL Workshop, 2018. [pdf](https://drive.google.com/open?id=0B_utB5Y8Y6D5d0NFZ25CdGluRDVGTlVyMHh2Q1g4NXZNbTJJ)
- J. Rothfuss, I. Clavera, J. Schulman, **Y. Fujita**, T. Asfour, and P. Abbeel, “Model-Based Reinforcement Learning via Meta-Policy Optimization,” in CoRL, 2018. [arXiv](https://arxiv.org/abs/1809.05214)
- **Y. Fujita** and S. Maeda, “Clipped Action Policy Gradient,” in ICML, 2018. [arXiv](https://arxiv.org/abs/1802.07564) [code](https://github.com/pfnet-research/capg) [slides](https://www.slideshare.net/mooopan/clipped-action-policy-gradient-107793858)

## Code

- [ChainerRL](https://github.com/chainer/chainerrl): A deep RL library in Python and Chainer
- [PFRL](https://github.com/pfnet/pfrl): A deep RL library in Python and PyTorch
- [async-rl](https://github.com/muupan/async-rl): An A3C implementation in Python and Chainer
- [DQN-in-the-Caffe](https://github.com/muupan/dqn-in-the-caffe): A DQN implementation in C++ and Caffe

## Work experience

- Engineer at Preferred Networks, Inc. (April 2015 - Present)

  - Research and development in machine learning for industrial applications including autonomous driving and robotics
  - Development of [ChainerRL](https://github.com/chainer/chainerrl) and [PFRL](https://github.com/pfnet/pfrl).
  
## Professional activities

- Program committee: Deep Reinforcement Learning Workshop at NeurIPS (2018-2020)
- Lecturer: RL part of 先端人工知能論II at the University of Tokyo (2016-2018)

## Education

- M.S. Information Science and Technology (April 2013 - March 2015)

  - Graduate School of Information Science and Technology, The University of Tokyo
  - Thesis: “Automatic Feature Generation and Model Learning for General Game Players Based on Reinforcement Learning“
  
- B.S Engineering (April 2011 - March 2013)
